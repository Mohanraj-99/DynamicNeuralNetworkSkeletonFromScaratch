{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DynamicNeuralNetworkSkeletonFromScaratch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-C4Jq0151dH7",
        "2PiRg6tW1hGA",
        "kxtqmAf319QC",
        "fkyTV2GY14cw",
        "Mfyc7tzv1p5u"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVtsK2Ln1Wju",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl5DIk-k0L3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modules\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbAlIQA20eoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dynamic params intialization of Weights abd Bias (First layer size depends on Input shape and Last layer size depends on Number_of_class)\n",
        "def init_params(layer_dims,num_of_class,X):\n",
        "  params = dict()\n",
        "  layer_dims.insert(0,X.shape[0]) \n",
        "  \n",
        "  for i in range(1,len(layer_dims)):\n",
        "    params['W'+str(i)] = np.random.randn(layer_dims[i],layer_dims[i-1]) * 0.01\n",
        "    params['b'+str(i)] = np.zeros((layer_dims[i],1))\n",
        "    #print(params['W'+str(i)].shape,params['b'+str(i)].shape)\n",
        "  \n",
        "  params['W'+str(i+1)] = np.random.randn(num_of_class,layer_dims[i]) * 0.01\n",
        "  params['b'+str(i+1)] = np.zeros((num_of_class,1))\n",
        "  #print(params['W'+str(i)].shape,params['b'+str(i)].shape)\n",
        "  \n",
        "  return params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C4Jq0151dH7",
        "colab_type": "text"
      },
      "source": [
        "# Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Dv_6b_04Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Few Activaion func for Forward pass\n",
        "def sigmoid(Z):\n",
        "  A = 1/(1+np.exp(-Z))\n",
        "  return A\n",
        "\n",
        "def relu(Z):\n",
        "  A = np.maximum(0,Z)\n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm8sHS7L1NHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A,W,b,activation):\n",
        "  Z = np.dot(W,A) + b\n",
        "  if activation == 'sigmoid':\n",
        "    A = sigmoid(Z)\n",
        "  if activation == 'relu':\n",
        "    A = relu(Z)\n",
        "  return A,Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioZr3Ez91RAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forwardpass(params,X,activation):\n",
        "  # Init of A and Z\n",
        "  A = dict()\n",
        "  Z = dict()\n",
        "  A['A'+str(0)] = X\n",
        "  for i in range(1,params//2):\n",
        "    A['A'+str(i)],Z['Z'+str(i)] = linear_forward(A['A'+str(i-1)],params['W']+str(i),params['b'+str(i)],activation)\n",
        "  AL,Z['Z'+str(i+1)] = linear_forward(A['A'+str(i)],params['W']+str(i+1),params['b'+str(i+1)],'sigmoid')\n",
        "  return AL,Z,A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PiRg6tW1hGA",
        "colab_type": "text"
      },
      "source": [
        "# Backward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC8pM04k1B7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Few Activaion func for Backward pass\n",
        "def sigmoid_backward(dA,Z):\n",
        "  s = 1/(1+np.exp(-Z))\n",
        "  dZ = dA * s * (1-s)\n",
        "  return dZ\n",
        "\n",
        "def relu_backward(dA,Z):\n",
        "  dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "  # When z <= 0, you should set dz to 0 as well. \n",
        "  dZ[Z <= 0] = 0\n",
        "  return dZ\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dEpKyz71mNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def liner_backward(dA,i,Z,A_prev,activation='sigmoid'):\n",
        "    # dZ:\n",
        "    if activation=='sigmoid':\n",
        "      dZ = sigmoid_backward(dA,Z)\n",
        "    if activation=='relu':\n",
        "      dZ = relu_backward(dA,Z)\n",
        "\n",
        "    m = A_prev.shape[1]\n",
        "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
        "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "    dA_prev = np.dot(W.T,dZ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFDQhqSY1pBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backwardpass(AL,Y,l,Z,A,activation): \n",
        "    # dZ depends on dA & Z (Using dA & Z - dZ is computed)\n",
        "    # dW,db depends on dZ (Using dZ - dW and db is computed)\n",
        "    grads = dict()\n",
        "    dAL = -1* ( np.divide( Y,AL ) + np.divide( (1-Y),(1-AL) ) )\n",
        "    #Last Layer alone\n",
        "    grads['dA'+str(l)],grads['dW'+str(l+1)],grads['db'+str(l+1)] = liner_backward(dAL,l,Z['Z'+str(l+1)],A['A'+str(l)])  \n",
        "    for i in range(l,0,-1):\n",
        "      grads['dA'+str(i-1)],grads['dW'+str(i)],grads['db'+str(i)] = liner_backward(grads['dA'+str(i)],i,Z['Z'+str(i)],A['A'+str(l-1)],activation)    \n",
        "    return grads    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxtqmAf319QC",
        "colab_type": "text"
      },
      "source": [
        "# Cost per Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhYPrj2D1zuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost_func(Y,AL,X):\n",
        "  curr_loss = ( -1/X.shape[1] ) * ( np.sum( np.multiply( Y,np.log(AL) ,axis=1) ) + np.sum(np.multiply( (1-Y),np.log(1-AL) ) ,axis=1) )\n",
        "  return curr_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkyTV2GY14cw",
        "colab_type": "text"
      },
      "source": [
        "# Update Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCuajzYw13ox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_params(params ,grads ,learning_rate):\n",
        "  for i in range(1,(params)//2+1):\n",
        "    params['W'+str(i)] = params['W'+str(i)] - learning_rate * grads['dW'+str(i)]\n",
        "    params['b'+str(i)] = params['b'+str(i)] - learning_rate * grads['db'+str(i)]\n",
        "  return params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfyc7tzv1p5u",
        "colab_type": "text"
      },
      "source": [
        "# Dynamic Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7su6AU9W1t1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main Neural network\n",
        "def linear_model(X,Y,learning_rate,anneal,activation,num_of_class,layer_dims,epochs):\n",
        "  # Init part \n",
        "  params = init_params(layer_dims,num_of_class,X)\n",
        "  l = len(layer_dims)\n",
        "  # costs over epochs\n",
        "  costs = []\n",
        "  # Epochs\n",
        "  for i in range(epochs):\n",
        "    # Forward Pass\n",
        "    AL,Z,A = linear_forwardpass(params,X,activation)\n",
        "    \n",
        "    # cost at each epoch\n",
        "    cost = cost_func(Y,AL,X)\n",
        "    \n",
        "    # Back Prop (Derivatives)\n",
        "    grads = linear_backwardpass(AL,Y,l,Z,A,activation)\n",
        "    \n",
        "    # Optimizer\n",
        "    # General Gradient descent\n",
        "\n",
        "    # Update params\n",
        "    params = update_params(params ,grads ,learning_rate)\n",
        "    \n",
        "    # anneal (Add)\n",
        "    if anneal == True:\n",
        "      learning_rate /= 2\n",
        "\n",
        "    # Loss over every 100 epochs\n",
        "    if i > 0  and i%100 == 0:\n",
        "      print('Cost at epoch is : {}'.format(cost))\n",
        "      costs.append(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}